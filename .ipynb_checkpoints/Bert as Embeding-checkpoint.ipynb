{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert as Embeding: Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "savvy searchers fail to spot ads internet search engine users are an odd mix of naive and sophisticated  suggests a report into search habits.  the report by the us pew research center reveals that 87% of searchers usually find what they were looking for when using a search engine. it also shows that few can spot the difference between paid-for results and organic ones. the report reveals that 84% of net users say they regularly use google  ask jeeves  msn and yahoo when online.  almost 50% of those questioned said they would trust search engines much less  if they knew information about who paid for results was being hidden. according to figures gathered by the pew researchers the average users spends about 43 minutes per month carrying out 34 separate searches and looks at 1.9 webpages for each hunt. a significant chunk of net users  36%  carry out a search at least weekly and 29% of those asked only look every few weeks. for 44% of those questioned  the information they are looking for is critical to what they are doing and is information they simply have to find.  search engine users also tend to be very loyal and once they have found a site they feel they can trust tend to stick with it. according to pew research 44% of searchers use just a single search engine  48% use two or three and a small number  7%  consult more than three sites. tony macklin  spokesman for ask jeeves  said the results reflected its own research which showed that people use different search engines because the way the sites gather information means they can provide different results for the same query. despite this liking for search sites half of those questioned said they could get the same information via other routes. a small number  17%  said they wouldn t really miss search engines if they did not exist. the remaining 33% said they could not live without search sites. more than two-thirds of those questioned  68%  said they thought that the results they were presented with were a fair and unbiased selection of the information on a topic that can be found on the net. alongside the growing sophistication of net users is a lack of awareness about paid-for results that many search engines provide alongside lists of websites found by indexing the web. of those asked  62% were unaware that someone has paid for some of the results they see when they carry out a search. only 18% of all searchers say they can tell which results are paid for and which are not. said the pew report:  this finding is ironic  since nearly half of all users say they would stop using search engines if they thought engines were not being clear about how they presented paid results.  commenting mr macklin said sponsored results must be clearly marked and though they might help with some queries user testing showed that people need to be able to spot the difference.\n",
      "\n",
      "step 1: Tokenize \n",
      "['sa', '##v', '##vy', 'search', '##ers', 'fail', 'to', 'spot', 'ads', 'internet', 'search', 'engine', 'users', 'are', 'an', 'odd', 'mix', 'of', 'naive', 'and', 'sophisticated', 'suggests', 'a', 'report', 'into', 'search', 'habits', '.', 'the', 'report', 'by', 'the', 'us', 'pew', 'research', 'center', 'reveals', 'that', '87', '%', 'of', 'search', '##ers', 'usually', 'find', 'what', 'they', 'were', 'looking', 'for', 'when', 'using', 'a', 'search', 'engine', '.', 'it', 'also', 'shows', 'that', 'few', 'can', 'spot', 'the', 'difference', 'between', 'paid', '-', 'for', 'results', 'and', 'organic', 'ones', '.', 'the', 'report', 'reveals', 'that', '84', '%', 'of', 'net', 'users', 'say', 'they', 'regularly', 'use', 'google', 'ask', 'je', '##eves', 'ms', '##n', 'and', 'yahoo', 'when', 'online', '.', 'almost', '50', '%', 'of', 'those', 'questioned', 'said', 'they', 'would', 'trust', 'search', 'engines', 'much', 'less', 'if', 'they', 'knew', 'information', 'about', 'who', 'paid', 'for', 'results', 'was', 'being', 'hidden', '.', 'according', 'to', 'figures', 'gathered', 'by', 'the', 'pew', 'researchers', 'the', 'average', 'users', 'spends', 'about', '43', 'minutes', 'per', 'month', 'carrying', 'out', '34', 'separate', 'searches', 'and', 'looks', 'at', '1', '.', '9', 'web', '##page', '##s', 'for', 'each', 'hunt', '.', 'a', 'significant', 'chunk', 'of', 'net', 'users', '36', '%', 'carry', 'out', 'a', 'search', 'at', 'least', 'weekly', 'and', '29', '%', 'of', 'those', 'asked', 'only', 'look', 'every', 'few', 'weeks', '.', 'for', '44', '%', 'of', 'those', 'questioned', 'the', 'information', 'they', 'are', 'looking', 'for', 'is', 'critical', 'to', 'what', 'they', 'are', 'doing', 'and', 'is', 'information', 'they', 'simply', 'have', 'to', 'find', '.', 'search', 'engine', 'users', 'also', 'tend', 'to', 'be', 'very', 'loyal', 'and', 'once', 'they', 'have', 'found', 'a', 'site', 'they', 'feel', 'they', 'can', 'trust', 'tend', 'to', 'stick', 'with', 'it', '.', 'according', 'to', 'pew', 'research', '44', '%', 'of', 'search', '##ers', 'use', 'just', 'a', 'single', 'search', 'engine', '48', '%', 'use', 'two', 'or', 'three', 'and', 'a', 'small', 'number', '7', '%', 'consult', 'more', 'than', 'three', 'sites', '.', 'tony', 'mack', '##lin', 'spokesman', 'for', 'ask', 'je', '##eves', 'said', 'the', 'results', 'reflected', 'its', 'own', 'research', 'which', 'showed', 'that', 'people', 'use', 'different', 'search', 'engines', 'because', 'the', 'way', 'the', 'sites', 'gather', 'information', 'means', 'they', 'can', 'provide', 'different', 'results', 'for', 'the', 'same', 'query', '.', 'despite', 'this', 'liking', 'for', 'search', 'sites', 'half', 'of', 'those', 'questioned', 'said', 'they', 'could', 'get', 'the', 'same', 'information', 'via', 'other', 'routes', '.', 'a', 'small', 'number', '17', '%', 'said', 'they', 'wouldn', 't', 'really', 'miss', 'search', 'engines', 'if', 'they', 'did', 'not', 'exist', '.', 'the', 'remaining', '33', '%', 'said', 'they', 'could', 'not', 'live', 'without', 'search', 'sites', '.', 'more', 'than', 'two', '-', 'thirds', 'of', 'those', 'questioned', '68', '%', 'said', 'they', 'thought', 'that', 'the', 'results', 'they', 'were', 'presented', 'with', 'were', 'a', 'fair', 'and', 'un', '##bia', '##sed', 'selection', 'of', 'the', 'information', 'on', 'a', 'topic', 'that', 'can', 'be', 'found', 'on', 'the', 'net', '.', 'alongside', 'the', 'growing', 'so', '##phi', '##stic', '##ation', 'of', 'net', 'users', 'is', 'a', 'lack', 'of', 'awareness', 'about', 'paid', '-', 'for', 'results', 'that', 'many', 'search', 'engines', 'provide', 'alongside', 'lists', 'of', 'websites', 'found', 'by', 'index', '##ing', 'the', 'web', '.', 'of', 'those', 'asked', '62', '%', 'were', 'unaware', 'that', 'someone', 'has', 'paid', 'for', 'some', 'of', 'the', 'results', 'they', 'see', 'when', 'they', 'carry', 'out', 'a', 'search', '.', 'only', '18', '%', 'of', 'all', 'search', '##ers', 'say', 'they', 'can', 'tell', 'which', 'results', 'are', 'paid', 'for', 'and', 'which', 'are', 'not', '.', 'said', 'the', 'pew', 'report', ':', 'this', 'finding', 'is', 'ironic', 'since', 'nearly', 'half', 'of', 'all', 'users', 'say', 'they', 'would', 'stop', 'using', 'search', 'engines', 'if', 'they', 'thought', 'engines', 'were', 'not', 'being', 'clear', 'about', 'how', 'they', 'presented', 'paid', 'results', '.', 'commenting', 'mr', 'mack', '##lin', 'said', 'sponsored', 'results', 'must', 'be', 'clearly', 'marked', 'and', 'though', 'they', 'might', 'help', 'with', 'some', 'que', '##ries', 'user', 'testing', 'showed', 'that', 'people', 'need', 'to', 'be', 'able', 'to', 'spot', 'the', 'difference', '.']\n",
      "\n",
      "step 2: adding special char: \n",
      "['[CLS]', 'sa', '##v', '##vy', 'search', '##ers', 'fail', 'to', 'spot', 'ads', 'internet', 'search', 'engine', 'users', 'are', 'an', 'odd', 'mix', 'of', 'naive', 'and', 'sophisticated', 'suggests', 'a', 'report', 'into', 'search', 'habits', '.', 'the', 'report', 'by', 'the', 'us', 'pew', 'research', 'center', 'reveals', 'that', '87', '%', 'of', 'search', '##ers', 'usually', 'find', 'what', 'they', 'were', 'looking', 'for', 'when', 'using', 'a', 'search', 'engine', '.', 'it', 'also', 'shows', 'that', 'few', 'can', 'spot', 'the', 'difference', 'between', 'paid', '-', 'for', 'results', 'and', 'organic', 'ones', '.', 'the', 'report', 'reveals', 'that', '84', '%', 'of', 'net', 'users', 'say', 'they', 'regularly', 'use', 'google', 'ask', 'je', '##eves', 'ms', '##n', 'and', 'yahoo', 'when', 'online', '.', 'almost', '50', '%', 'of', 'those', 'questioned', 'said', 'they', 'would', 'trust', 'search', 'engines', 'much', 'less', 'if', 'they', 'knew', 'information', 'about', 'who', 'paid', 'for', 'results', 'was', 'being', 'hidden', '.', 'according', 'to', 'figures', 'gathered', 'by', 'the', 'pew', 'researchers', 'the', 'average', 'users', 'spends', 'about', '43', 'minutes', 'per', 'month', 'carrying', 'out', '34', 'separate', 'searches', 'and', 'looks', 'at', '1', '.', '9', 'web', '##page', '##s', 'for', 'each', 'hunt', '.', 'a', 'significant', 'chunk', 'of', 'net', 'users', '36', '%', 'carry', 'out', 'a', 'search', 'at', 'least', 'weekly', 'and', '29', '%', 'of', 'those', 'asked', 'only', 'look', 'every', 'few', 'weeks', '.', 'for', '44', '%', 'of', 'those', 'questioned', 'the', 'information', 'they', 'are', 'looking', 'for', 'is', 'critical', 'to', 'what', 'they', 'are', 'doing', 'and', 'is', 'information', 'they', 'simply', 'have', 'to', 'find', '.', 'search', 'engine', 'users', 'also', 'tend', 'to', 'be', 'very', 'loyal', 'and', 'once', 'they', 'have', 'found', 'a', 'site', 'they', 'feel', 'they', 'can', 'trust', 'tend', 'to', 'stick', 'with', 'it', '.', 'according', 'to', 'pew', 'research', '44', '%', 'of', 'search', '##ers', 'use', 'just', 'a', 'single', 'search', 'engine', '48', '%', 'use', 'two', 'or', 'three', 'and', 'a', 'small', 'number', '7', '%', 'consult', 'more', 'than', 'three', 'sites', '.', 'tony', 'mack', '##lin', 'spokesman', 'for', 'ask', 'je', '##eves', 'said', 'the', 'results', 'reflected', 'its', 'own', 'research', 'which', 'showed', 'that', 'people', 'use', 'different', 'search', 'engines', 'because', 'the', 'way', 'the', 'sites', 'gather', 'information', 'means', 'they', 'can', 'provide', 'different', 'results', 'for', 'the', 'same', 'query', '.', 'despite', 'this', 'liking', 'for', 'search', 'sites', 'half', 'of', 'those', 'questioned', 'said', 'they', 'could', 'get', 'the', 'same', 'information', 'via', 'other', 'routes', '.', 'a', 'small', 'number', '17', '%', 'said', 'they', 'wouldn', 't', 'really', 'miss', 'search', 'engines', 'if', 'they', 'did', 'not', 'exist', '.', 'the', 'remaining', '33', '%', 'said', 'they', 'could', 'not', 'live', 'without', 'search', 'sites', '.', 'more', 'than', 'two', '-', 'thirds', 'of', 'those', 'questioned', '68', '%', 'said', 'they', 'thought', 'that', 'the', 'results', 'they', 'were', 'presented', 'with', 'were', 'a', 'fair', 'and', 'un', '##bia', '##sed', 'selection', 'of', 'the', 'information', 'on', 'a', 'topic', 'that', 'can', 'be', 'found', 'on', 'the', 'net', '.', 'alongside', 'the', 'growing', 'so', '##phi', '##stic', '##ation', 'of', 'net', 'users', 'is', 'a', 'lack', 'of', 'awareness', 'about', 'paid', '-', 'for', 'results', 'that', 'many', 'search', 'engines', 'provide', 'alongside', 'lists', 'of', 'websites', 'found', 'by', 'index', '##ing', 'the', 'web', '.', 'of', 'those', 'asked', '62', '%', 'were', 'unaware', 'that', 'someone', 'has', 'paid', 'for', 'some', 'of', 'the', 'results', 'they', 'see', 'when', 'they', 'carry', 'out', 'a', 'search', '.', 'only', '18', '%', 'of', 'all', 'search', '##ers', 'say', 'they', 'can', 'tell', 'which', 'results', 'are', 'paid', 'for', 'and', 'which', 'are', 'not', '.', 'said', 'the', 'pew', 'report', ':', 'this', 'finding', 'is', 'ironic', 'since', 'nearly', 'half', 'of', 'all', 'users', 'say', 'they', 'would', 'stop', 'using', 'search', 'engines', 'if', 'they', 'thought', 'engines', 'were', 'not', 'being', 'clear', 'about', 'how', 'they', 'presented', 'paid', 'results', '.', 'commenting', 'mr', 'mack', '##lin', 'said', 'sponsored', 'results', 'must', 'be', 'clearly', 'marked', 'and', 'though', 'they', 'might', 'help', 'with', 'some', 'que', '##ries', 'user', 'testing', 'showed', 'that', 'people', 'need', 'to', 'be', 'able', 'to', 'spot', 'the', 'difference', '.', '[SEP]']\n",
      "\n",
      "step 3: Padding: \n",
      "['[CLS]', 'sa', '##v', '##vy', 'search', '##ers', 'fail', 'to', 'spot', 'ads', 'internet', 'search', 'engine', 'users', 'are', 'an', 'odd', 'mix', 'of', 'naive', 'and', 'sophisticated', 'suggests', 'a', 'report', 'into', 'search', 'habits', '.', 'the', 'report', 'by', 'the', 'us', 'pew', 'research', 'center', 'reveals', 'that', '87', '%', 'of', 'search', '##ers', 'usually', 'find', 'what', 'they', 'were', 'looking', 'for', 'when', 'using', 'a', 'search', 'engine', '.', 'it', 'also', 'shows', 'that', 'few', 'can', 'spot', 'the', 'difference', 'between', 'paid', '-', 'for', 'results', 'and', 'organic', 'ones', '.', 'the', 'report', 'reveals', 'that', '84', '%', 'of', 'net', 'users', 'say', 'they', 'regularly', 'use', 'google', 'ask', 'je', '##eves', 'ms', '##n', 'and', 'yahoo', 'when', 'online', '.', 'almost', '50', '%', 'of', 'those', 'questioned', 'said', 'they', 'would', 'trust', 'search', 'engines', 'much', 'less', 'if', 'they', 'knew', 'information', 'about', 'who', 'paid', 'for', 'results', 'was', 'being', 'hidden', '.', 'according', 'to', 'figures', 'gathered', 'by', 'the', 'pew', 'researchers', 'the', 'average', 'users', 'spends', 'about', '43', 'minutes', 'per', 'month', 'carrying', 'out', '34', 'separate', 'searches', 'and', 'looks', 'at', '1', '.', '9', 'web', '##page', '##s', 'for', 'each', 'hunt', '.', 'a', 'significant', 'chunk', 'of', 'net', 'users', '36', '%', 'carry', 'out', 'a', 'search', 'at', 'least', 'weekly', 'and', '29', '%', 'of', 'those', 'asked', 'only', 'look', 'every', 'few', 'weeks', '.', 'for', '44', '%', 'of', 'those', 'questioned', 'the', 'information', 'they', 'are', 'looking', 'for', 'is', 'critical', 'to', 'what', 'they', 'are', 'doing', 'and', 'is', 'information', 'they', 'simply', 'have', 'to', 'find', '.', 'search', 'engine', 'users', 'also', 'tend', 'to', 'be', 'very', 'loyal', 'and', 'once', 'they', 'have', 'found', 'a', 'site', 'they', 'feel', 'they', 'can', 'trust', 'tend', 'to', 'stick', 'with', 'it', '.', 'according', 'to', 'pew', 'research', '44', '%', 'of', 'search', '##ers', 'use', 'just', 'a', '[SEP]']\n",
      "\n",
      "step 3: Attention Masking: \n",
      "['[CLS]', 'sa', '##v', '##vy', 'search', '##ers', 'fail', 'to', 'spot', 'ads', 'internet', 'search', 'engine', 'users', 'are', 'an', 'odd', 'mix', 'of', 'naive', 'and', 'sophisticated', 'suggests', 'a', 'report', 'into', 'search', 'habits', '.', 'the', 'report', 'by', 'the', 'us', 'pew', 'research', 'center', 'reveals', 'that', '87', '%', 'of', 'search', '##ers', 'usually', 'find', 'what', 'they', 'were', 'looking', 'for', 'when', 'using', 'a', 'search', 'engine', '.', 'it', 'also', 'shows', 'that', 'few', 'can', 'spot', 'the', 'difference', 'between', 'paid', '-', 'for', 'results', 'and', 'organic', 'ones', '.', 'the', 'report', 'reveals', 'that', '84', '%', 'of', 'net', 'users', 'say', 'they', 'regularly', 'use', 'google', 'ask', 'je', '##eves', 'ms', '##n', 'and', 'yahoo', 'when', 'online', '.', 'almost', '50', '%', 'of', 'those', 'questioned', 'said', 'they', 'would', 'trust', 'search', 'engines', 'much', 'less', 'if', 'they', 'knew', 'information', 'about', 'who', 'paid', 'for', 'results', 'was', 'being', 'hidden', '.', 'according', 'to', 'figures', 'gathered', 'by', 'the', 'pew', 'researchers', 'the', 'average', 'users', 'spends', 'about', '43', 'minutes', 'per', 'month', 'carrying', 'out', '34', 'separate', 'searches', 'and', 'looks', 'at', '1', '.', '9', 'web', '##page', '##s', 'for', 'each', 'hunt', '.', 'a', 'significant', 'chunk', 'of', 'net', 'users', '36', '%', 'carry', 'out', 'a', 'search', 'at', 'least', 'weekly', 'and', '29', '%', 'of', 'those', 'asked', 'only', 'look', 'every', 'few', 'weeks', '.', 'for', '44', '%', 'of', 'those', 'questioned', 'the', 'information', 'they', 'are', 'looking', 'for', 'is', 'critical', 'to', 'what', 'they', 'are', 'doing', 'and', 'is', 'information', 'they', 'simply', 'have', 'to', 'find', '.', 'search', 'engine', 'users', 'also', 'tend', 'to', 'be', 'very', 'loyal', 'and', 'once', 'they', 'have', 'found', 'a', 'site', 'they', 'feel', 'they', 'can', 'trust', 'tend', 'to', 'stick', 'with', 'it', '.', 'according', 'to', 'pew', 'research', '44', '%', 'of', 'search', '##ers', 'use', 'just', 'a', '[SEP]']\n",
      "\n",
      "step 4: Segment ids:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "step 5: BERT vocabulary index for each token:\n",
      "[101, 7842, 2615, 10736, 3945, 2545, 8246, 2000, 3962, 14997, 4274, 3945, 3194, 5198, 2024, 2019, 5976, 4666, 1997, 15743, 1998, 12138, 6083, 1037, 3189, 2046, 3945, 14243, 1012, 1996, 3189, 2011, 1996, 2149, 29071, 2470, 2415, 7657, 2008, 6584, 1003, 1997, 3945, 2545, 2788, 2424, 2054, 2027, 2020, 2559, 2005, 2043, 2478, 1037, 3945, 3194, 1012, 2009, 2036, 3065, 2008, 2261, 2064, 3962, 1996, 4489, 2090, 3825, 1011, 2005, 3463, 1998, 7554, 3924, 1012, 1996, 3189, 7657, 2008, 6391, 1003, 1997, 5658, 5198, 2360, 2027, 5570, 2224, 8224, 3198, 15333, 23047, 5796, 2078, 1998, 20643, 2043, 3784, 1012, 2471, 2753, 1003, 1997, 2216, 8781, 2056, 2027, 2052, 3404, 3945, 5209, 2172, 2625, 2065, 2027, 2354, 2592, 2055, 2040, 3825, 2005, 3463, 2001, 2108, 5023, 1012, 2429, 2000, 4481, 5935, 2011, 1996, 29071, 6950, 1996, 2779, 5198, 15970, 2055, 4724, 2781, 2566, 3204, 4755, 2041, 4090, 3584, 17193, 1998, 3504, 2012, 1015, 1012, 1023, 4773, 13704, 2015, 2005, 2169, 5690, 1012, 1037, 3278, 20000, 1997, 5658, 5198, 4029, 1003, 4287, 2041, 1037, 3945, 2012, 2560, 4882, 1998, 2756, 1003, 1997, 2216, 2356, 2069, 2298, 2296, 2261, 3134, 1012, 2005, 4008, 1003, 1997, 2216, 8781, 1996, 2592, 2027, 2024, 2559, 2005, 2003, 4187, 2000, 2054, 2027, 2024, 2725, 1998, 2003, 2592, 2027, 3432, 2031, 2000, 2424, 1012, 3945, 3194, 5198, 2036, 7166, 2000, 2022, 2200, 8884, 1998, 2320, 2027, 2031, 2179, 1037, 2609, 2027, 2514, 2027, 2064, 3404, 7166, 2000, 6293, 2007, 2009, 1012, 2429, 2000, 29071, 2470, 4008, 1003, 1997, 3945, 2545, 2224, 2074, 1037, 102]\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "\n",
    "#sentence = 'hehidden likes to play'\n",
    "sentence = data['text1'][0]\n",
    "print(sentence)\n",
    "\n",
    "max_len = 256\n",
    "\n",
    "# # Step 1: Tokenize\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(f\"\\nstep 1: Tokenize \\n{tokens}\")\n",
    "\n",
    "# # Step 2: Add [CLS] and [SEP]\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "print(f\"\\nstep 2: adding special char: \\n{tokens}\")\n",
    "\n",
    "# # Step 3: Pad tokens\n",
    "if len(tokens) < max_len:\n",
    "    padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "else:\n",
    "    padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "print(f\"\\nstep 3: Padding: \\n{padded_tokens}\")\n",
    "      \n",
    "attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "print(f\"\\nstep 3: Attention Masking: \\n{padded_tokens}\")  # Only give attention to non padded sentx\n",
    "\n",
    "# # Step 4: Segment ids\n",
    "seg_ids = [0 for _ in range(len(padded_tokens))] #Optional!\n",
    "print(f\"\\nstep 4: Segment ids:\\n{seg_ids}\") # Distinguish one sent to another (use case: question from paragraph)\n",
    "\n",
    "# # Step 5: Get BERT vocabulary index for each token\n",
    "token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "print(f\"\\nstep 5: BERT vocabulary index for each token:\\n{token_ids}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# # Convert to pytorch tensors\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "attn_mask = torch.tensor(attn_mask).unsqueeze(0)\n",
    "seg_ids = torch.tensor(seg_ids).unsqueeze(0)\n",
    "\n",
    "# # Feed them to bert\n",
    "hidden_reps, cls_head = bert_model(token_ids, attention_mask = attn_mask,\\\n",
    "                                   token_type_ids = seg_ids)\n",
    "print(hidden_reps.shape)\n",
    "print(cls_head.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class and Data Loaders\n",
    "\n",
    "#### ETL process --> convert data into perticular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename, maxlen):\n",
    "\n",
    "        # Store the contents of the file in a pandas dataframe\n",
    "        self.df = pd.read_csv(filename, delimiter=',')\n",
    "\n",
    "        # Initialize the BERT tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Define the Maxlength for padding/truncating\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'text1']\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "\n",
    "        # Inserting the CLS and SEP token at the beginning and end of the sentence\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        \n",
    "        # Padding/truncating the sentences to the maximum length\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]']\n",
    "        \n",
    "        # Convert the sequence to ids with BERT Vocabulary\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        # Converting the list to a pytorch tensor\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids)\n",
    "\n",
    "        # Obtaining the attention mask\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating instances of training and validation set\n",
    "dataset_text1 = LoadDataset(filename = 'Text_Similarity_Dataset.csv', maxlen = 256)\n",
    "\n",
    "# Creating intsances of training and validation dataloaders\n",
    "train_loader = DataLoader(dataset_text1, batch_size = 32, num_workers = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bert_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b702a29e017f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import mxnet as mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbert_embedding\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bert_embedding'"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "\n",
    "ctx = mx.gpu(0)\n",
    "bert = BertEmbedding(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_abstract = \"\"\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.\n",
    " Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.\n",
    " As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \n",
    "BERT is conceptually simple and empirically powerful. \n",
    "It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.\"\"\"\n",
    "sentences = bert_abstract.split('\\n')\n",
    "bert_embedding = BertEmbedding()\n",
    "result = bert_embedding(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = result[0]\n",
    "\n",
    "first_sentence[0]\n",
    "# ['we', 'introduce', 'a', 'new', 'language', 'representation', 'model', 'called', 'bert', ',', 'which', 'stands', 'for', 'bidirectional', 'encoder', 'representations', 'from', 'transformers']\n",
    "len(first_sentence[0])\n",
    "# 18\n",
    "\n",
    "\n",
    "len(first_sentence[1])\n",
    "# 18\n",
    "first_token_in_first_sentence = first_sentence[1]\n",
    "first_token_in_first_sentence[1]\n",
    "# array([ 0.4805648 ,  0.18369392, -0.28554988, ..., -0.01961522,\n",
    "#        1.0207764 , -0.67167974], dtype=float32)\n",
    "first_token_in_first_sentence[1].shape\n",
    "# (768,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
